{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8XpNMS7Vk6Df"
   },
   "source": [
    "# apillay -> HW4P2: Attention-based Speech Recognition\n",
    "\n",
    "### WanDB link: https://wandb.ai/audio-idl/hw4p2-ablations?workspace=user-apillay\n",
    "\n",
    "### Please find the following runs used to track the training for this work:\n",
    "v3090-66m (https://wandb.ai/audio-idl/hw4p2-ablations/runs/bejv4w15?workspace=user-apillay): Trained model from scratch\n",
    "v3090-66m-resume (https://wandb.ai/audio-idl/hw4p2-ablations/runs/jvqqfyeq?workspace=user-apillay): Trained from previous modelâ€™s last epoch checkpoint with the following changes:\n",
    "                                                                                                    - Weight Decay changed from 1e-2 to 1e-3\n",
    "                                                                                                    - Dropout p changed from 0.5 to 0.2\n",
    "                                                                                                    - Frequency masking factor changed from 3 to 5\n",
    "                                                                                                    - Time masking probability changed from 0.5 to 0.75\n",
    "                                                                                                    - Learning rate changed from 2e-4 to 6e-5\n",
    "                                                                                                    - ReduceLROnPlateau reduction factor changed from 0.75 to 0.8\n",
    "                                                                                                    - Teacher forcing rate changed from 1.0 to 0.5\n",
    "\n",
    "### Instructions to run the code: Run all cells in the following sequence!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8UK7J-dp5iN5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri May  5 17:21:55 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:62:00.0 Off |                  N/A |\n",
      "|  0%   53C    P8    40W / 300W |      6MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n",
      "\n",
      "==============NVSMI LOG==============\n",
      "\n",
      "Timestamp                                 : Fri May  5 17:21:56 2023\n",
      "Driver Version                            : 525.105.17\n",
      "CUDA Version                              : 12.0\n",
      "\n",
      "Attached GPUs                             : 1\n",
      "GPU 00000000:62:00.0\n",
      "    Power Readings\n",
      "        Power Management                  : Supported\n",
      "        Power Draw                        : 29.97 W\n",
      "        Power Limit                       : 300.00 W\n",
      "        Default Power Limit               : 420.00 W\n",
      "        Enforced Power Limit              : 300.00 W\n",
      "        Min Power Limit                   : 100.00 W\n",
      "        Max Power Limit                   : 450.00 W\n",
      "    Power Samples\n",
      "        Duration                          : 47.64 sec\n",
      "        Number of Samples                 : 119\n",
      "        Max                               : 135.27 W\n",
      "        Min                               : 27.62 W\n",
      "        Avg                               : 35.48 W\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "!nvidia-smi -q -d POWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nYgaLmgy5iqR"
   },
   "outputs": [],
   "source": [
    "# Install some required libraries\n",
    "# Feel free to add more if you want\n",
    "!pip install -q python-levenshtein torchsummaryX wandb kaggle pytorch-nlp seaborn torchaudio torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-get install zip unzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yEkA_GGG-tTB"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:17:40.430445060Z",
     "start_time": "2023-05-03T03:17:38.403427992Z"
    },
    "id": "p0aXmrxM-usO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence, pad_sequence\n",
    "import torchaudio.transforms as tat\n",
    "import torchnlp.nn\n",
    "from torchinfo import summary\n",
    "import Levenshtein\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device: \", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-njBvl2Opd6I"
   },
   "source": [
    "# Kaggle Dataset Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PTyWR2sIp0Ns"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8\n",
    "!rm -rf /root/.kaggle\n",
    "!mkdir /root/.kaggle\n",
    "\n",
    "with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n",
    "    f.write('{\"username\":\"ashwinpillay\",\"key\":\"<key>\"}')\n",
    "    # Put your kaggle username & key here\n",
    "\n",
    "!chmod 600 /root/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F581gjfnqE2C"
   },
   "outputs": [],
   "source": [
    "# To download the dataset\n",
    "!kaggle datasets download -d varunjain3/11-785-s23-hw4p2-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ko7QN16qF2V"
   },
   "outputs": [],
   "source": [
    "# To unzip data quickly and quietly\n",
    "!unzip -q 11-785-s23-hw4p2-dataset.zip -d ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:17:46.538128753Z",
     "start_time": "2023-05-03T03:17:46.529433950Z"
    }
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'batch_size': 128,\n",
    "    'lr': 6e-5,\n",
    "    'tf_rate': 0.5,\n",
    "    'tf_decay_factor': 0.9,\n",
    "    'tf_cooldown_period': 15,\n",
    "    'epochs': 500,\n",
    "    'max_timesteps': 550,\n",
    "    'rlrop_params': {\n",
    "        'factor': 0.8,\n",
    "        'patience': 4,\n",
    "        'threshold': 1,\n",
    "        'threshold_mode': 'rel',\n",
    "        'cooldown': 2,\n",
    "    },\n",
    "    'CA_lr_params': {\n",
    "        'T_max': 8,\n",
    "        'eta_min': 1e-6,\n",
    "    },\n",
    "    'dropout_p': 0.2,\n",
    "    'adamw_weight_decay': 1e-3,\n",
    "    'time_mask_maxl': 50,\n",
    "    'time_mask_p': 0.75,\n",
    "    'freq_mask_maxl': 5,\n",
    "    'num_workers_train': 6,\n",
    "    'num_workers_val': 4,\n",
    "    'num_workers_test': 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zUJyBBwIqQs6"
   },
   "source": [
    "# Dataset and Dataloaders\n",
    "\n",
    "We have given you 2 datasets. One is a toy dataset, and the other is the standard LibriSpeech dataset. The toy dataset is to help you get your code implemented and tested and debugged easily, to verify that your attention diagonal is produced correctly. Note however that it's task (phonetic transcription) is drawn from HW3P2, it is meant to be familiar and help you understand how to transition from phonetic transcription to alphabet transcription, with a working attention module.\n",
    "\n",
    "Please make sure you use the right constants in your code implementation for future modules, (SOS_TOKEN vs SOS_TOKEN_TOY) when working with either dataset. We have defined the constants accordingly below. Before you come to OH or post on piazza, make sure you aren't misuing the constants for either dataset in your code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1sjsbFuKWp7q"
   },
   "source": [
    "## LibriSpeech\n",
    "\n",
    "In terms of the dataset, the dataset structure for HW3P2 and HW4P2 dataset are very similar. Can you spot out the differences? What all will be required?? \n",
    "\n",
    "Hints:\n",
    "\n",
    "- Check how big is the dataset (do you require memory efficient loading techniques??)\n",
    "- How do we load mfccs? Do we need to normalise them? \n",
    "- Does the data have \\<SOS> and \\<EOS> tokens in each sequences? Do we remove them or do we not remove them? (Read writeup)\n",
    "- Would we want a collating function? Ask yourself: Why did we need a collate function last time?\n",
    "- Observe the VOCAB, is the dataset same as HW3P2? \n",
    "- Should you add augmentations, if yes which augmentations? When should you add augmentations? (Check bootcamp for answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:17:55.527273418Z",
     "start_time": "2023-05-03T03:17:55.519907633Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MBMLGYX-kZcd",
    "outputId": "90d67714-0574-4557-e9f3-15a4fcf8a7de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of vocab: 31\n",
      "Vocab: ['<pad>', '<sos>', '<eos>', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', \"'\", ' ']\n",
      "PAD_TOKEN: 0\n",
      "SOS_TOKEN: 1\n",
      "EOS_TOKEN: 2\n"
     ]
    }
   ],
   "source": [
    "VOCAB = ['<pad>', '<sos>', '<eos>',\n",
    "         'A',   'B',    'C',    'D',    \n",
    "         'E',   'F',    'G',    'H',    \n",
    "         'I',   'J',    'K',    'L',       \n",
    "         'M',   'N',    'O',    'P',    \n",
    "         'Q',   'R',    'S',    'T', \n",
    "         'U',   'V',    'W',    'X', \n",
    "         'Y',   'Z',    \"'\",    ' ', \n",
    "         ]\n",
    "\n",
    "VOCAB_MAP = {VOCAB[i]:i for i in range(0, len(VOCAB))}\n",
    "\n",
    "PAD_TOKEN = VOCAB_MAP[\"<pad>\"]\n",
    "SOS_TOKEN = VOCAB_MAP[\"<sos>\"]\n",
    "EOS_TOKEN = VOCAB_MAP[\"<eos>\"]\n",
    "\n",
    "print(f\"Length of vocab: {len(VOCAB)}\")\n",
    "print(f\"Vocab: {VOCAB}\")\n",
    "print(f\"PAD_TOKEN: {PAD_TOKEN}\")\n",
    "print(f\"SOS_TOKEN: {SOS_TOKEN}\")\n",
    "print(f\"EOS_TOKEN: {EOS_TOKEN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:17:57.251505412Z",
     "start_time": "2023-05-03T03:17:57.243574608Z"
    }
   },
   "outputs": [],
   "source": [
    "# Utils for network\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "class PermuteBlock(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:17:58.782574619Z",
     "start_time": "2023-05-03T03:17:58.777193596Z"
    }
   },
   "outputs": [],
   "source": [
    "def cepstral_normalize(mfcc):\n",
    "    mean_mfcc = np.mean(mfcc, axis=0)\n",
    "    std_mfcc = np.std(mfcc, axis=0) + np.finfo('float').eps\n",
    "    normalized_mfcc = (mfcc - mean_mfcc) / std_mfcc\n",
    "    return normalized_mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:18:00.233362686Z",
     "start_time": "2023-05-03T03:18:00.231310736Z"
    },
    "id": "VuneWaTStdF2"
   },
   "outputs": [],
   "source": [
    "class SpeechDatasetTrain(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, root1='.', root2=None):\n",
    "        '''\n",
    "        Initializes the dataset.\n",
    "        '''\n",
    "\n",
    "        self.mfcc_paths, self.transcript_paths = [], []\n",
    "        self.VOCAB = VOCAB\n",
    "\n",
    "        self.augmentations = torch.nn.Sequential(\n",
    "            PermuteBlock(),\n",
    "            tat.TimeMasking(time_mask_param=config['time_mask_maxl'], p=config['time_mask_p']),\n",
    "            tat.FrequencyMasking(freq_mask_param=config['freq_mask_maxl']),\n",
    "            PermuteBlock(),\n",
    "        )\n",
    "\n",
    "        roots = [root1]\n",
    "        if root2 is not None:\n",
    "            roots.append(root2)\n",
    "\n",
    "        for root in roots:\n",
    "            print(\"=> Loading paths from: {}\".format(root))\n",
    "            mfcc_dir = os.path.join(root, 'mfcc')\n",
    "            transcript_dir = os.path.join(root, 'transcripts')\n",
    "\n",
    "            mfcc_files = sorted(os.listdir(mfcc_dir))\n",
    "            transcript_files = sorted(os.listdir(transcript_dir))\n",
    "\n",
    "            root_file_length = len(mfcc_files)\n",
    "            for i in tqdm(range(root_file_length)):\n",
    "                self.mfcc_paths.append(os.path.join(root, 'mfcc', mfcc_files[i]))\n",
    "                self.transcript_paths.append(os.path.join(root, 'transcripts', transcript_files[i]))\n",
    "        # Train Dataset only initializes with path to reduce CPU RAM usage (empirically found that fetching data during __getitem__ is not too costly!\n",
    "\n",
    "        assert len(self.mfcc_paths) == len(self.transcript_paths)\n",
    "        self.length = len(self.mfcc_paths)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        '''\n",
    "        Return a tuple of features and labels.\n",
    "        '''\n",
    "        return self.mfcc_paths[ind], self.transcript_paths[ind]\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        '''\n",
    "        1.  Extract the features and labels from 'batch'\n",
    "        2.  We will additionally need to pad both features and labels,\n",
    "            look at pytorch's docs for pad_sequence\n",
    "        3.  This is a good place to perform transforms, if you so wish.\n",
    "            Performing them on batches will speed the process up a bit.\n",
    "        4.  Return batch of features, labels, lenghts of features,\n",
    "            and lengths of labels.\n",
    "        '''\n",
    "\n",
    "        # batch of input mfcc coefficients\n",
    "        batch_mfcc_paths = [bitem[0] for bitem in batch]\n",
    "\n",
    "        # batch of output phonemes\n",
    "        batch_transcript_paths = [bitem[1] for bitem in batch]\n",
    "\n",
    "        batch_mfcc, batch_transcript = [], []\n",
    "        for mffc_ind in range(len(batch_mfcc_paths)):\n",
    "            mfcc = np.load(batch_mfcc_paths[mffc_ind])\n",
    "            mfcc = cepstral_normalize(mfcc)\n",
    "            mfcc = torch.FloatTensor(mfcc)\n",
    "            batch_mfcc.append(mfcc)\n",
    "\n",
    "            transcript = np.load(batch_transcript_paths[mffc_ind])[1:-1]\n",
    "            transcript_maps = [self.VOCAB.index(j) for j in transcript]\n",
    "            transcript = torch.tensor(transcript_maps)\n",
    "            batch_transcript.append(transcript)\n",
    "\n",
    "        lengths_mfcc = [len(mfcc_i) for mfcc_i in batch_mfcc]\n",
    "\n",
    "        lengths_transcript = [len(transcript_i) for transcript_i in batch_transcript]\n",
    "\n",
    "        batch_mfcc_pad = pad_sequence(batch_mfcc, batch_first=True, padding_value=EOS_TOKEN)\n",
    "        batch_transcript_pad = pad_sequence(batch_transcript, batch_first=True, padding_value=EOS_TOKEN)\n",
    "\n",
    "        batch_mfcc_pad = self.augmentations(batch_mfcc_pad)\n",
    "\n",
    "        # Return the following values: padded features, padded labels, actual length of features, actual length of the labels\n",
    "        return batch_mfcc_pad, batch_transcript_pad, torch.tensor(lengths_mfcc), torch.tensor(lengths_transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:18:03.002338155Z",
     "start_time": "2023-05-03T03:18:02.993969652Z"
    }
   },
   "outputs": [],
   "source": [
    "class SpeechDatasetValidate(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, root1='.', root2=None):\n",
    "        '''\n",
    "        Initializes the dataset.\n",
    "\n",
    "        '''\n",
    "\n",
    "        # Load the directory and all files in them\n",
    "\n",
    "        self.mfccs, self.transcripts = [], []\n",
    "        self.VOCAB = VOCAB\n",
    "\n",
    "        roots = [root1]\n",
    "        if root2 is not None:\n",
    "            roots.append(root2)\n",
    "\n",
    "        for root in roots:\n",
    "            print(\"=> Loading from: {}\".format(root))\n",
    "            mfcc_dir = os.path.join(root, 'mfcc')\n",
    "            transcript_dir = os.path.join(root, 'transcripts')\n",
    "\n",
    "            mfcc_files = sorted(os.listdir(mfcc_dir))\n",
    "            transcript_files = sorted(os.listdir(transcript_dir))\n",
    "\n",
    "            root_file_length = len(mfcc_files)\n",
    "            for i in tqdm(range(root_file_length)):\n",
    "                mfcc = np.load(os.path.join(root, 'mfcc', mfcc_files[i]))\n",
    "                mfcc = cepstral_normalize(mfcc)\n",
    "                self.mfccs.append(mfcc)\n",
    "\n",
    "                transcript = np.load(os.path.join(root, 'transcripts', transcript_files[i]))[1:-1]\n",
    "                transcript_maps = [self.VOCAB.index(j) for j in transcript]\n",
    "                self.transcripts.append(transcript_maps)\n",
    "\n",
    "        assert len(self.mfccs) == len(self.transcripts)\n",
    "        self.length = len(self.mfccs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        '''\n",
    "        Return a tuple of features and labels.\n",
    "        '''\n",
    "        mfcc = torch.FloatTensor(self.mfccs[ind])\n",
    "        transcript = torch.tensor(self.transcripts[ind])\n",
    "        return mfcc, transcript\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        '''\n",
    "        1.  Extract the features and labels from 'batch'\n",
    "        2.  We will additionally need to pad both features and labels,\n",
    "            look at pytorch's docs for pad_sequence\n",
    "        3.  This is a good place to perform transforms, if you so wish.\n",
    "            Performing them on batches will speed the process up a bit.\n",
    "        4.  Return batch of features, labels, lenghts of features,\n",
    "            and lengths of labels.\n",
    "        '''\n",
    "        # batch of input mfcc coefficients\n",
    "\n",
    "        batch_mfcc = [bitem[0] for bitem in batch]\n",
    "\n",
    "        # batch of output phonemes\n",
    "        batch_transcript = [bitem[1] for bitem in batch]\n",
    "\n",
    "        batch_mfcc_pad = pad_sequence(batch_mfcc, batch_first=True, padding_value=EOS_TOKEN)\n",
    "        lengths_mfcc = [len(mfcc_i) for mfcc_i in batch_mfcc]\n",
    "\n",
    "        batch_transcript_pad = pad_sequence(batch_transcript, batch_first=True, padding_value=EOS_TOKEN)\n",
    "        lengths_transcript = [len(transcript_i) for transcript_i in batch_transcript]\n",
    "\n",
    "        # Return the following values: padded features, padded labels, actual length of features, actual length of the labels\n",
    "        return batch_mfcc_pad, batch_transcript_pad, torch.tensor(lengths_mfcc), torch.tensor(lengths_transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:18:05.293415028Z",
     "start_time": "2023-05-03T03:18:05.287774752Z"
    }
   },
   "outputs": [],
   "source": [
    "class SpeechDatasetTest(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, root='.'):\n",
    "        '''\n",
    "        Initializes the dataset.\n",
    "\n",
    "        INPUTS: What inputs do you need here?\n",
    "        '''\n",
    "\n",
    "        # Load the directory and all files in them\n",
    "\n",
    "        self.mfcc_dir = os.path.join(root, 'mfcc')\n",
    "\n",
    "        self.mfcc_files = sorted(os.listdir(self.mfcc_dir))\n",
    "\n",
    "        self.length = len(self.mfcc_files)\n",
    "\n",
    "        self.mfccs = []\n",
    "        for i in tqdm(range(self.length)):\n",
    "            mfcc = np.load(os.path.join(root, 'mfcc', self.mfcc_files[i]))\n",
    "            mfcc = cepstral_normalize(mfcc)\n",
    "            self.mfccs.append(mfcc)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        '''\n",
    "        Return a tuple of features and labels.\n",
    "        '''\n",
    "\n",
    "        mfcc = torch.FloatTensor(self.mfccs[ind])\n",
    "        return mfcc\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        '''\n",
    "        1.  Extract the features and labels from 'batch'\n",
    "        2.  We will additionally need to pad both features and labels,\n",
    "            look at pytorch's docs for pad_sequence\n",
    "        3.  This is a good place to perform transforms, if you so wish.\n",
    "            Performing them on batches will speed the process up a bit.\n",
    "        4.  Return batch of features, labels, lenghts of features,\n",
    "            and lengths of labels.\n",
    "        '''\n",
    "        # batch of input mfcc coefficients\n",
    "\n",
    "        batch_mfcc = batch\n",
    "\n",
    "        batch_mfcc_pad = pad_sequence(batch_mfcc, batch_first=True, padding_value=EOS_TOKEN)\n",
    "        lengths_mfcc = [len(mfcc_i) for mfcc_i in batch_mfcc]\n",
    "\n",
    "        # Return the following values: padded features, actual length of features\n",
    "        return batch_mfcc_pad, torch.tensor(lengths_mfcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:18:09.450867440Z",
     "start_time": "2023-05-03T03:18:07.345126105Z"
    },
    "id": "tzuIXCyAuNvo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Train Dataset: \n",
      "=> Loading paths from: ./data/train-clean-100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28539/28539 [00:00<00:00, 275852.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Val Dataset: \n",
      "=> Loading from: ./data/dev-clean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2703/2703 [00:01<00:00, 1643.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Test Dataset: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2620/2620 [00:00<00:00, 2855.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch size:  128\n",
      "Train dataset samples = 28539, batches = 223\n",
      "Val dataset samples = 2703, batches = 22\n",
      "Test dataset samples = 2620, batches = 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "\n",
    "# Create objects for the dataset class\n",
    "print(\"Loading Train Dataset: \")\n",
    "train_data = SpeechDatasetTrain(root1='./data/train-clean-100')\n",
    "\n",
    "print(\"Loading Val Dataset: \")\n",
    "val_data = SpeechDatasetValidate(root1='./data/dev-clean')\n",
    "\n",
    "print(\"Loading Test Dataset: \")\n",
    "test_data = SpeechDatasetTest(root='./data/test-clean')\n",
    "\n",
    "# Do NOT forget to pass in the collate function as parameter while creating the dataloader\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_data,\n",
    "    collate_fn=train_data.collate_fn,\n",
    "    num_workers=config['num_workers_train'],\n",
    "    batch_size=config['batch_size'],\n",
    "    pin_memory=True,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    dataset=val_data,\n",
    "    collate_fn=val_data.collate_fn,\n",
    "    num_workers=config['num_workers_val'],\n",
    "    batch_size=config['batch_size'],\n",
    "    pin_memory=True,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_data,\n",
    "    collate_fn=test_data.collate_fn,\n",
    "    num_workers=config['num_workers_test'],\n",
    "    batch_size=config['batch_size'],\n",
    "    pin_memory=True,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(\"\\nBatch size: \", config['batch_size'])\n",
    "print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
    "print(\"Val dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\n",
    "print(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:18:10.855330890Z",
     "start_time": "2023-05-03T03:18:10.854285989Z"
    }
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:18:13.728513092Z",
     "start_time": "2023-05-03T03:18:12.053995080Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1634, 27]) torch.Size([128, 300]) torch.Size([128]) torch.Size([128])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for data in train_loader:\n",
    "    x, y, lx, ly = data\n",
    "    print(x.shape, y.shape, lx.shape, ly.shape)\n",
    "    # Plots train data with its augmentations\n",
    "    plt.imshow(x[0].T, interpolation='nearest', aspect='auto', cmap='viridis')\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# THE MODEL\n",
    "\n",
    "### Listen, Attend and Spell\n",
    "Listen, Attend and Spell (LAS) is a neural network model used for speech recognition and synthesis tasks.\n",
    "\n",
    "- LAS is designed to handle long input sequences and is robust to noisy speech signals.\n",
    "- LAS is known for its high accuracy and ability to improve over time with additional training data.\n",
    "- It consists of an <b>listener, an attender and a speller</b>, which work together to convert an input speech signal into a corresponding output text.\n",
    "\n",
    "#### The Listener:\n",
    "- converts the input speech signal into a sequence of hidden states.\n",
    "\n",
    "#### The Attender:\n",
    "- Decides how the sequence of Encoder hidden state is propogated to decoder.\n",
    "\n",
    "#### The Speller:\n",
    "- A language model, that incorporates the \"context of attender\"(output of attender) to predict sequence of words.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The Listener:\n",
    "### Borrowed Liberally from the encoder of my HW3P2 submission"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:18:17.940127811Z",
     "start_time": "2023-05-03T03:18:17.933650062Z"
    }
   },
   "outputs": [],
   "source": [
    "class Resnet34ResidualBlock1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, kernel_size=3, padding=1):\n",
    "        super(Resnet34ResidualBlock1d, self).__init__()\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.cnn2 = nn.Sequential(\n",
    "            nn.Conv1d(out_channels, out_channels, kernel_size, 1, padding, bias=False),\n",
    "            nn.BatchNorm1d(out_channels)\n",
    "        )\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "        else:\n",
    "            self.shortcut = nn.Sequential()\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.cnn1(x)\n",
    "        x = self.cnn2(x)\n",
    "        x += self.shortcut(residual)\n",
    "        x = nn.ReLU(True)(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:18:19.637133283Z",
     "start_time": "2023-05-03T03:18:19.632284662Z"
    }
   },
   "outputs": [],
   "source": [
    "class pBLSTM(nn.Module):\n",
    "    '''\n",
    "    Pyramidal BiLSTM\n",
    "    Read the writeup/paper and understand the concepts and then write your implementation here.\n",
    "\n",
    "    At each step,\n",
    "    1. Pad your input if it is packed (Unpack it)\n",
    "    2. Reduce the input length dimension by concatenating feature dimension\n",
    "        (Tip: Write down the shapes and understand)\n",
    "        (i) How should  you deal with odd/even length input?\n",
    "        (ii) How should you deal with input length array (x_lens) after truncating the input?\n",
    "    3. Pack your input\n",
    "    4. Pass it into LSTM layer\n",
    "\n",
    "    To make our implementation modular, we pass 1 layer at a time.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, n_lstm=1):\n",
    "        super(pBLSTM, self).__init__()\n",
    "\n",
    "        self.ld = torchnlp.nn.LockedDropout(p=config['dropout_p'])\n",
    "\n",
    "        self.ln = nn.LayerNorm(input_size * 2, eps=1e-06)\n",
    "\n",
    "        self.blstm = nn.LSTM(input_size=input_size * 2, hidden_size=hidden_size, num_layers=n_lstm, bidirectional=True,\n",
    "                             batch_first=True)\n",
    "\n",
    "    def forward(self, x_packed):  # x_packed is a PackedSequence\n",
    "\n",
    "        # Pad Packed Sequence\n",
    "        x, x_lens = pad_packed_sequence(x_packed, batch_first=True, padding_value=0.0)\n",
    "\n",
    "        x, x_lens = self.trunc_reshape(x, x_lens)\n",
    "\n",
    "        x = torch.permute(x, (1, 0, 2))\n",
    "        x = self.ld(x)\n",
    "        x = self.ln(x)\n",
    "        x = torch.permute(x, (1, 0, 2))\n",
    "\n",
    "        # Pack Padded Sequence.\n",
    "        x = pack_padded_sequence(x, lengths=x_lens, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        # Pass the sequence through bLSTM\n",
    "        x, _ = self.blstm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def trunc_reshape(self, x, x_lens):\n",
    "        if x.size(dim=1) % 2 == 1:\n",
    "            x = x[:, :-1, :]\n",
    "        x = x.reshape(x.shape[0], x.shape[1] // 2, x.shape[2] * 2)\n",
    "        x_lens //= 2\n",
    "        return x, x_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:18:21.658617610Z",
     "start_time": "2023-05-03T03:18:21.650688691Z"
    }
   },
   "outputs": [],
   "source": [
    "class Listener(nn.Module):\n",
    "    '''\n",
    "    The Encoder takes utterances as inputs and returns latent feature representations\n",
    "    '''\n",
    "\n",
    "    def __init__(self, input_feature_size=27, encoder_hidden_size=256):\n",
    "        super(Listener, self).__init__()\n",
    "\n",
    "        self.embedding_stride = 1\n",
    "        self.embedding_kernel_size = 3\n",
    "        self.embedding_padding = 1\n",
    "\n",
    "        self.embedding = nn.Sequential(\n",
    "            PermuteBlock(),\n",
    "            Resnet34ResidualBlock1d(in_channels=input_feature_size, out_channels=1024, stride=self.embedding_stride,\n",
    "                                    kernel_size=self.embedding_kernel_size, padding=self.embedding_padding),\n",
    "            PermuteBlock(),\n",
    "        )\n",
    "\n",
    "        self.pBLSTMs = nn.Sequential(\n",
    "            pBLSTM(input_size=1024, hidden_size=encoder_hidden_size, n_lstm=2),\n",
    "            pBLSTM(input_size=encoder_hidden_size * 2, hidden_size=encoder_hidden_size, n_lstm=2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, x_lens):\n",
    "        # Call the embedding layer\n",
    "        x = self.embedding(x)\n",
    "        # x_lens = (x_lens - self.embedding_kernel_size + 2 * self.embedding_padding) // self.embedding_stride\n",
    "\n",
    "        # Pack Padded Sequence\n",
    "        x = pack_padded_sequence(x, lengths=x_lens, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        # Pass Sequence through the pyramidal Bi-LSTM layer\n",
    "        x = self.pBLSTMs(x)\n",
    "\n",
    "        # Pad Packed Sequence\n",
    "        encoder_outputs, encoder_lens = pad_packed_sequence(x, batch_first=True, padding_value=0.0)\n",
    "\n",
    "        return encoder_outputs, encoder_lens"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Attention"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "771TXxn7ViOW"
   },
   "outputs": [],
   "source": [
    "# Dot Product Attention:\n",
    "class Attention(nn.Module):\n",
    "    '''\n",
    "    Attention is calculated using the key, value (from encoder embeddings) and query from decoder.\n",
    "\n",
    "    After obtaining the raw weights, compute and return attention weights and context as follows.:\n",
    "\n",
    "    attention_weights   = softmax(raw_weights)\n",
    "    attention_context   = einsum(\"thinkwhatwouldbetheequationhere\",attention, value) #take hint from raw_weights calculation\n",
    "\n",
    "    At the end, you can pass context through a linear layer too.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, encoder_hidden_size, decoder_hidden_size, projection_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.VW = nn.Linear(in_features=encoder_hidden_size * 2, out_features=projection_size)\n",
    "        self.KW = nn.Linear(in_features=encoder_hidden_size * 2, out_features=projection_size)\n",
    "        self.QW = nn.Linear(in_features=decoder_hidden_size, out_features=projection_size)\n",
    "\n",
    "    def set_key_value(self, encoder_outputs):\n",
    "        '''\n",
    "        In this function we take the encoder embeddings and make key and values from it.\n",
    "        key.shape   = (batch_size, timesteps, projection_size)\n",
    "        value.shape = (batch_size, timesteps, projection_size)\n",
    "        '''\n",
    "        self.key = self.KW(encoder_outputs)\n",
    "        self.value = self.VW(encoder_outputs)\n",
    "\n",
    "    def compute_context(self, decoder_context):\n",
    "        '''\n",
    "        In this function from decoder context, we make the query, and then we\n",
    "         multiply the queries with the keys to find the attention logits,\n",
    "         finally we take a softmax to calculate attention energy which gets\n",
    "         multiplied to the generted values and then gets summed.\n",
    "\n",
    "        key.shape   = (batch_size, timesteps, projection_size) -> (B, P, T)\n",
    "        value.shape = (batch_size, timesteps, projection_size)\n",
    "        query.shape = (batch_size, 1, projection_size) -> (B, 1, P)\n",
    "\n",
    "        You are also recomended to check out Abu's Lecture 19 to understand Attention better.\n",
    "        '''\n",
    "\n",
    "        query = self.QW(decoder_context)  # (batch_size, 1, projection_size)\n",
    "\n",
    "        # Using bmm or einsum. We need to perform batch matrix multiplication. It is important you do this step correctly.\n",
    "        # What will be the shape of raw_weights?\n",
    "        raw_weights = torch.bmm(query, self.key.mT) / (query.shape[1] ** 0.5)  # (B, 1, T)\n",
    "\n",
    "        # Raw_weights -> attention_weights\n",
    "        attention_weights = nn.functional.softmax(raw_weights, dim=2)  # over dim=T\n",
    "\n",
    "        # Multiply attention weights to values\n",
    "        attention_context = torch.bmm(attention_weights, self.value)\n",
    "\n",
    "        return attention_context, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-02T19:14:42.379942316Z",
     "start_time": "2023-05-02T19:14:42.372367901Z"
    },
    "id": "4Sp1WywZmm1L"
   },
   "source": [
    "## The Speller\n",
    "\n",
    "Similar to the language model that you coded up for HW4P1, you have to code a language model for HW4P2 as well. This time, we will also call theattention context step, within the decoder to get the attended-encoder-embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derived from: https://pytorchnlp.readthedocs.io/en/latest/_modules/torchnlp/nn/lock_dropout.html\n",
    "class LockedDropoutCell(nn.Module):\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "        super().__init__()\n",
    "        self.mask = None\n",
    "\n",
    "    def create_mask(self, x):\n",
    "        mask = x.new_empty(x.size(0), x.size(1), requires_grad=False).bernoulli_(1 - self.p)\n",
    "        mask = mask.div_(1 - self.p)\n",
    "        self.mask = mask.expand_as(x)\n",
    "\n",
    "    def delete_mask(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.training or not self.p:\n",
    "            return x\n",
    "\n",
    "        if self.mask is None:\n",
    "            self.create_mask(x.clone())\n",
    "        return x * self.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:37:45.452156984Z",
     "start_time": "2023-05-03T03:37:45.407981964Z"
    },
    "id": "nFkc6MbnlUPu"
   },
   "outputs": [],
   "source": [
    "class Speller(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, attender: Attention, vocab_size, encoder_hidden_size, decoder_embedding_size=256,\n",
    "                 attention_projection_size=128, decoder_hidden_size=256, max_timesteps=550):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder_hidden_size = encoder_hidden_size\n",
    "\n",
    "        self.max_timesteps = max_timesteps\n",
    "\n",
    "        self.attend = attender  # Attention object in speller\n",
    "\n",
    "        # Embedding layer to convert token to latent space\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=decoder_embedding_size)\n",
    "\n",
    "        # Sequence of LSTM Cells\n",
    "        self.lstm_cells = nn.Sequential(\n",
    "            nn.LSTMCell(input_size=decoder_embedding_size + attention_projection_size, hidden_size=decoder_hidden_size),\n",
    "            nn.LSTMCell(input_size=decoder_hidden_size, hidden_size=decoder_hidden_size),\n",
    "            nn.LSTMCell(input_size=decoder_hidden_size, hidden_size=decoder_hidden_size),\n",
    "            nn.LSTMCell(input_size=decoder_hidden_size, hidden_size=decoder_hidden_size),\n",
    "        )\n",
    "\n",
    "        self.lds = nn.Sequential(\n",
    "            LockedDropoutCell(p=config['dropout_p']),\n",
    "            LockedDropoutCell(p=config['dropout_p']),\n",
    "            LockedDropoutCell(p=config['dropout_p']),\n",
    "            LockedDropoutCell(p=config['dropout_p'])\n",
    "        )\n",
    "\n",
    "        # For CDN (Feel free to change)\n",
    "\n",
    "        # Linear module to convert outputs to correct hidden size\n",
    "        # + Linear layer to convert hidden space back to logits for token classification\n",
    "        self.char_prob = nn.Sequential(\n",
    "            nn.Linear(in_features=(decoder_hidden_size + attention_projection_size),\n",
    "                      out_features=decoder_embedding_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(in_features=decoder_embedding_size, out_features=vocab_size))\n",
    "\n",
    "        # Weight tying (From embedding layer)\n",
    "        self.char_prob[2].weight = self.embedding.weight\n",
    "        self.attention_projection_size = attention_projection_size\n",
    "\n",
    "    def lstm_step(self, input_word, hidden_state):\n",
    "        for i in range(len(self.lstm_cells)):\n",
    "            # Forward pass through each LSTMCell\n",
    "            hidden_state[i] = self.lstm_cells[i](input_word, hidden_state[i])\n",
    "            input_word = hidden_state[i][0]\n",
    "            input_word = self.lds[i](input_word)  # Keeps last lstm cell's output over that timestep\n",
    "        return input_word, hidden_state\n",
    "\n",
    "    def forward(self, batch_size, y=None, teacher_forcing_ratio=1):\n",
    "        # initial context tensor for time t = 0\n",
    "        attn_context = torch.zeros((batch_size, self.attention_projection_size)).to(DEVICE)\n",
    "\n",
    "        output_symbol = torch.zeros((batch_size,)).long().to(DEVICE)\n",
    "        output_symbol[:, ] = SOS_TOKEN  # Set it to SOS for time t = 0\n",
    "\n",
    "        raw_outputs = []\n",
    "        attention_plot = []\n",
    "\n",
    "        if y is None:\n",
    "            timesteps = self.max_timesteps\n",
    "            teacher_forcing_ratio = 0  # Why does it become zero?\n",
    "        else:\n",
    "            # Timesteps we are predicting for\n",
    "            timesteps = y.shape[1]\n",
    "\n",
    "        # Initializing hidden_states list\n",
    "        hidden_states_list = [None] * len(self.lstm_cells)\n",
    "\n",
    "        for ld in self.lds:\n",
    "            ld.delete_mask()\n",
    "\n",
    "        for t in range(timesteps):\n",
    "            p = np.random.random()  # generate a probability p between 0 and 1\n",
    "\n",
    "            # Take from y, else draw from probability distribution\n",
    "            if p < teacher_forcing_ratio and t > 0:\n",
    "                output_symbol = y[:, t - 1]\n",
    "\n",
    "            # Embed the character symbol\n",
    "            char_embed = self.embedding(output_symbol.long())\n",
    "\n",
    "            # Concatenate the character embedding and context from attention, as shown in the diagram\n",
    "            lstm_input = torch.concatenate((char_embed, attn_context), dim=1)\n",
    "\n",
    "            # Feed the input through LSTM Cells and attention.\n",
    "            rnn_out, hidden_states_list = self.lstm_step(lstm_input, hidden_states_list)\n",
    "\n",
    "            # Feed the resulting hidden state into attention\n",
    "            attn_context, attn_weights = self.attend.compute_context(rnn_out.unsqueeze(dim=1))\n",
    "            attn_context = attn_context.squeeze(dim=1)\n",
    "\n",
    "            # Concatenating the context from the attention module with the LSTM output hidden state\n",
    "            cdn_input = torch.concatenate((rnn_out, attn_context), dim=1)\n",
    "\n",
    "            # Calling CDN with cdn_input\n",
    "            raw_pred = self.char_prob(cdn_input.squeeze(dim=1))\n",
    "\n",
    "            # Generate a prediction for this timestep and collect it in output_symbols\n",
    "            output_symbol = torch.argmax(raw_pred, dim=1)\n",
    "\n",
    "            raw_outputs.append(raw_pred)  # for loss calculation\n",
    "            attention_plot.append(attn_weights)  # for plotting attention plot\n",
    "\n",
    "        attention_plot = torch.stack(attention_plot, dim=1)\n",
    "        raw_outputs = torch.stack(raw_outputs, dim=1)\n",
    "\n",
    "        return raw_outputs, attention_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bgOQlDRI-E4z"
   },
   "source": [
    "## LAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-02T19:14:45.484432877Z",
     "start_time": "2023-05-02T19:14:45.481111577Z"
    },
    "id": "ZuAjQFlTBVED"
   },
   "source": [
    "Here we finally build the LAS model, comibining the listener, attender and speller together, we have given a template, but you are free to read the paper and implement it yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:37:54.433308927Z",
     "start_time": "2023-05-03T03:37:54.428531579Z"
    },
    "id": "scvB2cI-OSof"
   },
   "outputs": [],
   "source": [
    "class LAS(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, encoder_hidden_size, decoder_hidden_size, decoder_embedding_size,\n",
    "                 attention_projection_size,\n",
    "                 max_timesteps):  # add parameters\n",
    "        super().__init__()\n",
    "\n",
    "        # Pass the right parameters here\n",
    "        self.listener = Listener(input_feature_size=27,\n",
    "                                 encoder_hidden_size=encoder_hidden_size)\n",
    "        self.attend = Attention(encoder_hidden_size=encoder_hidden_size,\n",
    "                                decoder_hidden_size=decoder_hidden_size,\n",
    "                                projection_size=attention_projection_size)\n",
    "        self.speller = Speller(attender=self.attend,\n",
    "                               vocab_size=vocab_size,\n",
    "                               encoder_hidden_size=encoder_hidden_size,\n",
    "                               decoder_embedding_size=decoder_embedding_size,\n",
    "                               attention_projection_size=attention_projection_size,\n",
    "                               decoder_hidden_size=decoder_hidden_size,\n",
    "                               max_timesteps=max_timesteps)\n",
    "\n",
    "    def forward(self, x, lx, y=None, teacher_forcing_ratio=1):\n",
    "        # Encode speech features\n",
    "        encoder_outputs, _ = self.listener(x, lx)\n",
    "\n",
    "        # We want to compute keys and values ahead of the decoding step, as they are constant for all timesteps\n",
    "        # Set keys and values using the encoder outputs\n",
    "        self.attend.set_key_value(encoder_outputs)\n",
    "\n",
    "        # Decode text with the speller using context from the attention\n",
    "        raw_outputs, attention_plots = self.speller(batch_size=x.shape[0], y=y, teacher_forcing_ratio=teacher_forcing_ratio)\n",
    "\n",
    "        return raw_outputs, attention_plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bPZD3vqdUisj"
   },
   "source": [
    "# Model Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:37:58.443219358Z",
     "start_time": "2023-05-03T03:37:57.214852608Z"
    },
    "id": "a9LN0l5VUk_s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAS(\n",
      "  (listener): Listener(\n",
      "    (embedding): Sequential(\n",
      "      (0): PermuteBlock()\n",
      "      (1): Resnet34ResidualBlock1d(\n",
      "        (cnn1): Sequential(\n",
      "          (0): Conv1d(27, 1024, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "          (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (cnn2): Sequential(\n",
      "          (0): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "          (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (shortcut): Sequential(\n",
      "          (0): Conv1d(27, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
      "          (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (2): PermuteBlock()\n",
      "    )\n",
      "    (pBLSTMs): Sequential(\n",
      "      (0): pBLSTM(\n",
      "        (ld): LockedDropout(p=0.2)\n",
      "        (ln): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "        (blstm): LSTM(2048, 640, num_layers=2, batch_first=True, bidirectional=True)\n",
      "      )\n",
      "      (1): pBLSTM(\n",
      "        (ld): LockedDropout(p=0.2)\n",
      "        (ln): LayerNorm((2560,), eps=1e-06, elementwise_affine=True)\n",
      "        (blstm): LSTM(2560, 640, num_layers=2, batch_first=True, bidirectional=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (attend): Attention(\n",
      "    (VW): Linear(in_features=1280, out_features=512, bias=True)\n",
      "    (KW): Linear(in_features=1280, out_features=512, bias=True)\n",
      "    (QW): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (speller): Speller(\n",
      "    (attend): Attention(\n",
      "      (VW): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (KW): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (QW): Linear(in_features=512, out_features=512, bias=True)\n",
      "    )\n",
      "    (embedding): Embedding(31, 512)\n",
      "    (lstm_cells): Sequential(\n",
      "      (0): LSTMCell(1024, 512)\n",
      "      (1): LSTMCell(512, 512)\n",
      "      (2): LSTMCell(512, 512)\n",
      "      (3): LSTMCell(512, 512)\n",
      "    )\n",
      "    (lds): Sequential(\n",
      "      (0): LockedDropoutCell()\n",
      "      (1): LockedDropoutCell()\n",
      "      (2): LockedDropoutCell()\n",
      "      (3): LockedDropoutCell()\n",
      "    )\n",
      "    (char_prob): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): Linear(in_features=512, out_features=31, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "======================================================================\n",
       "Layer (type:depth-idx)                        Param #\n",
       "======================================================================\n",
       "LAS                                           --\n",
       "â”œâ”€Listener: 1-1                               --\n",
       "â”‚    â””â”€Sequential: 2-1                        --\n",
       "â”‚    â”‚    â””â”€PermuteBlock: 3-1                 --\n",
       "â”‚    â”‚    â””â”€Resnet34ResidualBlock1d: 3-2      3,262,464\n",
       "â”‚    â”‚    â””â”€PermuteBlock: 3-3                 --\n",
       "â”‚    â””â”€Sequential: 2-2                        --\n",
       "â”‚    â”‚    â””â”€pBLSTM: 3-4                       23,617,536\n",
       "â”‚    â”‚    â””â”€pBLSTM: 3-5                       26,240,000\n",
       "â”œâ”€Attention: 1-2                              --\n",
       "â”‚    â””â”€Linear: 2-3                            655,872\n",
       "â”‚    â””â”€Linear: 2-4                            655,872\n",
       "â”‚    â””â”€Linear: 2-5                            262,656\n",
       "â”œâ”€Speller: 1-3                                1,558,528\n",
       "â”‚    â””â”€Attention: 2-6                         (recursive)\n",
       "â”‚    â”‚    â””â”€Linear: 3-6                       (recursive)\n",
       "â”‚    â”‚    â””â”€Linear: 3-7                       (recursive)\n",
       "â”‚    â”‚    â””â”€Linear: 3-8                       (recursive)\n",
       "â”‚    â””â”€Embedding: 2-7                         15,872\n",
       "â”‚    â””â”€Sequential: 2-8                        --\n",
       "â”‚    â”‚    â””â”€LSTMCell: 3-9                     3,149,824\n",
       "â”‚    â”‚    â””â”€LSTMCell: 3-10                    2,101,248\n",
       "â”‚    â”‚    â””â”€LSTMCell: 3-11                    2,101,248\n",
       "â”‚    â”‚    â””â”€LSTMCell: 3-12                    2,101,248\n",
       "â”‚    â””â”€Sequential: 2-9                        --\n",
       "â”‚    â”‚    â””â”€LockedDropoutCell: 3-13           --\n",
       "â”‚    â”‚    â””â”€LockedDropoutCell: 3-14           --\n",
       "â”‚    â”‚    â””â”€LockedDropoutCell: 3-15           --\n",
       "â”‚    â”‚    â””â”€LockedDropoutCell: 3-16           --\n",
       "â”‚    â””â”€Sequential: 2-10                       --\n",
       "â”‚    â”‚    â””â”€Linear: 3-17                      524,800\n",
       "â”‚    â”‚    â””â”€GELU: 3-18                        --\n",
       "â”‚    â”‚    â””â”€Linear: 3-19                      15,903\n",
       "======================================================================\n",
       "Total params: 66,263,071\n",
       "Trainable params: 66,263,071\n",
       "Non-trainable params: 0\n",
       "======================================================================"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LAS(\n",
    "    vocab_size=len(VOCAB),\n",
    "    encoder_hidden_size=640,\n",
    "    decoder_hidden_size=512,\n",
    "    decoder_embedding_size=512,\n",
    "    attention_projection_size=512,\n",
    "    max_timesteps=config['max_timesteps']\n",
    ")\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "print(model)\n",
    "\n",
    "x = x.float().to(DEVICE)\n",
    "y = y.to(DEVICE)\n",
    "\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "23DMfXsaU6kj"
   },
   "source": [
    "# Loss Function, Optimizers, Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:38:05.732943931Z",
     "start_time": "2023-05-03T03:38:05.724317757Z"
    },
    "id": "216ukmHbU-ol"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr'], weight_decay=config['adamw_weight_decay'])\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='mean', ignore_index=PAD_TOKEN)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, **config['rlrop_params'])\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-02T19:14:51.281582751Z",
     "start_time": "2023-05-02T19:14:51.275889991Z"
    },
    "id": "ZWQnB8lUVY4f"
   },
   "source": [
    "# Levenshtein Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:18:54.431004828Z",
     "start_time": "2023-05-03T03:18:54.428431300Z"
    },
    "id": "rSsiCdxPVeZW"
   },
   "outputs": [],
   "source": [
    "# We have given you this utility function which takes a sequence of indices and converts them to a list of characters\n",
    "def indices_to_chars(indices, vocab):\n",
    "    tokens = []\n",
    "    for i in indices: # This loops through all the indices\n",
    "        if int(i) == SOS_TOKEN: # If SOS is encountered, dont add it to the final list\n",
    "            continue\n",
    "        elif int(i) == EOS_TOKEN: # If EOS is encountered, stop the decoding process\n",
    "            break\n",
    "        else:\n",
    "            tokens.append(vocab[int(i)])\n",
    "    return tokens\n",
    "\n",
    "# To make your life more easier, we have given the Levenshtein distantce / Edit distance calculation code\n",
    "def calc_edit_distance(predictions, y, ly, vocab= VOCAB, print_example= False):\n",
    "\n",
    "    dist                = 0\n",
    "    batch_size, seq_len = predictions.shape\n",
    "\n",
    "    for batch_idx in range(batch_size): \n",
    "\n",
    "        y_sliced    = indices_to_chars(y[batch_idx,0:ly[batch_idx]], vocab)\n",
    "        pred_sliced = indices_to_chars(predictions[batch_idx], vocab)\n",
    "\n",
    "        # Strings - When you are using characters from the AudioDataset\n",
    "        y_string    = ''.join(y_sliced)\n",
    "        pred_string = ''.join(pred_sliced)\n",
    "        \n",
    "        dist        += Levenshtein.distance(pred_string, y_string)\n",
    "        # Comment the above and uncomment below for toy dataset, as the toy dataset has a list of phonemes to compare\n",
    "        # dist      += Levenshtein.distance(y_sliced, pred_sliced)\n",
    "\n",
    "    if print_example: \n",
    "        # Print y_sliced and pred_sliced if you are using the toy dataset\n",
    "        print(\"Ground Truth : \", y_string)\n",
    "        print(\"Prediction   : \", pred_string)\n",
    "        \n",
    "    dist/=batch_size\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-02T19:15:08.765234674Z",
     "start_time": "2023-05-02T19:14:53.371365196Z"
    },
    "id": "Pu4MrSMUUIyp"
   },
   "source": [
    "# Train and Validation functions \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:38:09.118982954Z",
     "start_time": "2023-05-03T03:38:09.077662193Z"
    },
    "id": "sKOdI0J5Tpem"
   },
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer, teacher_forcing_rate):\n",
    "    model.train()\n",
    "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_perplexity = 0.0\n",
    "\n",
    "    for i, (x, y, lx, ly) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            raw_predictions, attention_plot = model(x, lx, y=y, teacher_forcing_ratio=teacher_forcing_rate)\n",
    "\n",
    "            outs = raw_predictions.reshape(-1, raw_predictions.shape[-1])\n",
    "            y = y.reshape(-1).long()\n",
    "            loss = criterion(outs, y)\n",
    "\n",
    "            perplexity = torch.exp(loss)  # Perplexity is defined the exponential of the loss\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            running_perplexity += perplexity.item()\n",
    "\n",
    "        # Backward on the masked loss\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        batch_bar.set_postfix(\n",
    "            loss=\"{:.04f}\".format(running_loss / (i + 1)),\n",
    "            perplexity=\"{:.04f}\".format(running_perplexity / (i + 1)),\n",
    "            lr=\"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])),\n",
    "            tf_rate='{:.02f}'.format(teacher_forcing_rate))\n",
    "        batch_bar.update()\n",
    "\n",
    "        del x, y, lx, ly, outs, raw_predictions\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    running_loss /= len(dataloader)\n",
    "    running_perplexity /= len(dataloader)\n",
    "    batch_bar.close()\n",
    "\n",
    "    return running_loss, running_perplexity, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:25:55.409588357Z",
     "start_time": "2023-05-03T03:25:55.386102381Z"
    },
    "id": "YmBLhP8cWm6n"
   },
   "outputs": [],
   "source": [
    "def validate(model, dataloader):\n",
    "    model.eval()\n",
    "\n",
    "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc=\"Val\")\n",
    "\n",
    "    running_lev_dist = 0.0\n",
    "    for i, (x, y, lx, ly) in enumerate(dataloader):\n",
    "        x, y, lx, ly = x.to(DEVICE), y.to(DEVICE), lx, ly\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            raw_predictions, _ = model(x, lx, y=None)\n",
    "\n",
    "        # Greedy Decoding\n",
    "        # Get the most likely character from each distribution in the batch\n",
    "        greedy_predictions = torch.argmax(raw_predictions, dim=2)\n",
    "\n",
    "        # Calculate Levenshtein Distance\n",
    "        if i == len(dataloader) - 1:\n",
    "            running_lev_dist += calc_edit_distance(greedy_predictions, y, ly, VOCAB, print_example=True)\n",
    "        else:   \n",
    "            running_lev_dist += calc_edit_distance(greedy_predictions, y, ly, VOCAB, print_example=False)\n",
    "\n",
    "        batch_bar.set_postfix(\n",
    "            dist=\"{:.04f}\".format(running_lev_dist / (i + 1)))\n",
    "        batch_bar.update()\n",
    "\n",
    "        del x, y, lx, ly, raw_predictions, greedy_predictions\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    batch_bar.close()\n",
    "    running_lev_dist /= len(dataloader)\n",
    "\n",
    "    return running_lev_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JmZhxhNseaIr"
   },
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "sZcCV2BIW2R6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mapillay\u001B[0m (\u001B[33maudio-idl\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login(key=\"<key>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/wandb/run-20230505_172239-jvqqfyeq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/audio-idl/hw4p2-ablations/runs/jvqqfyeq' target=\"_blank\">v3090-66m-resume</a></strong> to <a href='https://wandb.ai/audio-idl/hw4p2-ablations' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/audio-idl/hw4p2-ablations' target=\"_blank\">https://wandb.ai/audio-idl/hw4p2-ablations</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/audio-idl/hw4p2-ablations/runs/jvqqfyeq' target=\"_blank\">https://wandb.ai/audio-idl/hw4p2-ablations/runs/jvqqfyeq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    name=\"v3090-66m-resume\",  ## Wandb creates random run names if you skip this field\n",
    "    reinit=True,  ### Allows reinitalizing runs when you re-run this cell\n",
    "    # run_id = ### Insert specific run id here if you want to resume a previous run\n",
    "    # resume = \"must\" ### You need this to resume previous runs, but comment out reinit = True when using this\n",
    "    project=\"hw4p2-ablations\",  ### Project should be created in your wandb account\n",
    "    config=config  ### Wandb Config for your run\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:19:02.831685507Z",
     "start_time": "2023-05-03T03:19:02.822572909Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, scheduler, metric, epoch, path):\n",
    "    torch.save(\n",
    "        {'model_state_dict': model.state_dict(),\n",
    "         'optimizer_state_dict': optimizer.state_dict(),\n",
    "         'scheduler_state_dict': scheduler.state_dict(),\n",
    "         metric[0]: metric[1],\n",
    "         'epoch': epoch},\n",
    "        path\n",
    "    )\n",
    "\n",
    "\n",
    "def load_model(path, model, metric='val_ld', optimizer=None, scheduler=None):\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    if optimizer != None:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    if scheduler != None:\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "\n",
    "    epoch = checkpoint['epoch']\n",
    "    metric = checkpoint[metric]\n",
    "\n",
    "    return [model, optimizer, scheduler, epoch, metric]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:19:05.336285429Z",
     "start_time": "2023-05-03T03:19:05.328248184Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is for checkpointing, if you're doing it over multiple sessions\n",
    "\n",
    "last_epoch_completed = 0\n",
    "start = last_epoch_completed\n",
    "end = config[\"epochs\"]\n",
    "min_val_ld = float(\"inf\")  # if you're restarting from some checkpoint, use what you saw there.\n",
    "epoch_model_path = './checkpoint_epoch.pth'\n",
    "best_model_path = './checkpoint_best.pth'\n",
    "epoch_model_path_66m = './checkpoint_epoch_66m.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAS(\n",
      "  (listener): Listener(\n",
      "    (embedding): Sequential(\n",
      "      (0): PermuteBlock()\n",
      "      (1): Resnet34ResidualBlock1d(\n",
      "        (cnn1): Sequential(\n",
      "          (0): Conv1d(27, 1024, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "          (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (cnn2): Sequential(\n",
      "          (0): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "          (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (shortcut): Sequential(\n",
      "          (0): Conv1d(27, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
      "          (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (2): PermuteBlock()\n",
      "    )\n",
      "    (pBLSTMs): Sequential(\n",
      "      (0): pBLSTM(\n",
      "        (ld): LockedDropout(p=0.2)\n",
      "        (ln): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "        (blstm): LSTM(2048, 640, num_layers=2, batch_first=True, bidirectional=True)\n",
      "      )\n",
      "      (1): pBLSTM(\n",
      "        (ld): LockedDropout(p=0.2)\n",
      "        (ln): LayerNorm((2560,), eps=1e-06, elementwise_affine=True)\n",
      "        (blstm): LSTM(2560, 640, num_layers=2, batch_first=True, bidirectional=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (attend): Attention(\n",
      "    (VW): Linear(in_features=1280, out_features=512, bias=True)\n",
      "    (KW): Linear(in_features=1280, out_features=512, bias=True)\n",
      "    (QW): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (speller): Speller(\n",
      "    (attend): Attention(\n",
      "      (VW): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (KW): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (QW): Linear(in_features=512, out_features=512, bias=True)\n",
      "    )\n",
      "    (embedding): Embedding(31, 512)\n",
      "    (lstm_cells): Sequential(\n",
      "      (0): LSTMCell(1024, 512)\n",
      "      (1): LSTMCell(512, 512)\n",
      "      (2): LSTMCell(512, 512)\n",
      "      (3): LSTMCell(512, 512)\n",
      "    )\n",
      "    (lds): Sequential(\n",
      "      (0): LockedDropoutCell()\n",
      "      (1): LockedDropoutCell()\n",
      "      (2): LockedDropoutCell()\n",
      "      (3): LockedDropoutCell()\n",
      "    )\n",
      "    (char_prob): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): Linear(in_features=512, out_features=31, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "======================================================================\n",
       "Layer (type:depth-idx)                        Param #\n",
       "======================================================================\n",
       "LAS                                           --\n",
       "â”œâ”€Listener: 1-1                               --\n",
       "â”‚    â””â”€Sequential: 2-1                        --\n",
       "â”‚    â”‚    â””â”€PermuteBlock: 3-1                 --\n",
       "â”‚    â”‚    â””â”€Resnet34ResidualBlock1d: 3-2      3,262,464\n",
       "â”‚    â”‚    â””â”€PermuteBlock: 3-3                 --\n",
       "â”‚    â””â”€Sequential: 2-2                        --\n",
       "â”‚    â”‚    â””â”€pBLSTM: 3-4                       23,617,536\n",
       "â”‚    â”‚    â””â”€pBLSTM: 3-5                       26,240,000\n",
       "â”œâ”€Attention: 1-2                              --\n",
       "â”‚    â””â”€Linear: 2-3                            655,872\n",
       "â”‚    â””â”€Linear: 2-4                            655,872\n",
       "â”‚    â””â”€Linear: 2-5                            262,656\n",
       "â”œâ”€Speller: 1-3                                1,558,528\n",
       "â”‚    â””â”€Attention: 2-6                         (recursive)\n",
       "â”‚    â”‚    â””â”€Linear: 3-6                       (recursive)\n",
       "â”‚    â”‚    â””â”€Linear: 3-7                       (recursive)\n",
       "â”‚    â”‚    â””â”€Linear: 3-8                       (recursive)\n",
       "â”‚    â””â”€Embedding: 2-7                         15,872\n",
       "â”‚    â””â”€Sequential: 2-8                        --\n",
       "â”‚    â”‚    â””â”€LSTMCell: 3-9                     3,149,824\n",
       "â”‚    â”‚    â””â”€LSTMCell: 3-10                    2,101,248\n",
       "â”‚    â”‚    â””â”€LSTMCell: 3-11                    2,101,248\n",
       "â”‚    â”‚    â””â”€LSTMCell: 3-12                    2,101,248\n",
       "â”‚    â””â”€Sequential: 2-9                        --\n",
       "â”‚    â”‚    â””â”€LockedDropoutCell: 3-13           --\n",
       "â”‚    â”‚    â””â”€LockedDropoutCell: 3-14           --\n",
       "â”‚    â”‚    â””â”€LockedDropoutCell: 3-15           --\n",
       "â”‚    â”‚    â””â”€LockedDropoutCell: 3-16           --\n",
       "â”‚    â””â”€Sequential: 2-10                       --\n",
       "â”‚    â”‚    â””â”€Linear: 3-17                      524,800\n",
       "â”‚    â”‚    â””â”€GELU: 3-18                        --\n",
       "â”‚    â”‚    â””â”€Linear: 3-19                      15,903\n",
       "======================================================================\n",
       "Total params: 66,263,071\n",
       "Trainable params: 66,263,071\n",
       "Non-trainable params: 0\n",
       "======================================================================"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# del model\n",
    "model, _, _, _, min_val_ld = load_model(epoch_model_path, model, 'val_ld')\n",
    "print(model)\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:19:06.861252671Z",
     "start_time": "2023-05-03T03:19:06.849276702Z"
    },
    "id": "IgJdA9jrZwid"
   },
   "outputs": [],
   "source": [
    "def plot_attention(attention):\n",
    "    # Function for plotting attention matrix\n",
    "    plt.clf()\n",
    "    attention_matrix = sns.heatmap(attention, cmap='GnBu')\n",
    "    fig = attention_matrix.get_figure()\n",
    "    plt.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:19:08.789265585Z",
     "start_time": "2023-05-03T03:19:08.745571069Z"
    }
   },
   "outputs": [],
   "source": [
    "def step_teacher_forcing_rate(epoch, teacher_forcing_rate):\n",
    "    if epoch % (config['tf_cooldown_period']-1) == 0:\n",
    "        return teacher_forcing_rate * config['tf_decay_factor']\n",
    "    return teacher_forcing_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:39:47.936991124Z",
     "start_time": "2023-05-03T03:38:15.623065410Z"
    },
    "id": "eDWGFIcjddz-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 157/223 [07:00<02:53,  2.63s/it, loss=0.0714, lr=0.0001, perplexity=1.0751, tf_rate=0.50]"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "teacher_forcing_rate = config['tf_rate']\n",
    "for epoch in range(0, config['epochs']):\n",
    "\n",
    "    print(\"\\nEpoch: {}/{}\".format(epoch + 1, config['epochs']))\n",
    "\n",
    "    curr_lr = float(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "    train_loss, train_perplexity, attention_plot = train(model, train_loader, criterion, optimizer, teacher_forcing_rate)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    print(\n",
    "        \"Epoch {}/{}: Train Loss {:.04f}\\t Train Perplexity {:.04f}\\t LR = {:.04f}\\t Teacher Forcing Rate {:.04f}\".format(\n",
    "            epoch + 1,\n",
    "            config['epochs'],\n",
    "            train_loss,\n",
    "            train_perplexity,\n",
    "            curr_lr,\n",
    "            teacher_forcing_rate))\n",
    "\n",
    "    val_ld = validate(model, val_loader)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"\\nVal LD {:.04f}\\t\".format(val_ld))\n",
    "\n",
    "    # Call train and validate, get attention weights from training\n",
    "\n",
    "    # Print your metrics\n",
    "\n",
    "    # Plot Attention for a single item in the batch\n",
    "    attention_fig = plot_attention(attention_plot[0].squeeze().cpu().detach().numpy())\n",
    "\n",
    "    # Log metrics to Wandb\n",
    "    wandb.log({\n",
    "        'Train LD': train_loss,\n",
    "        'Train Perplexity': train_perplexity,\n",
    "        'Valid LD': val_ld,\n",
    "        'LR': curr_lr,\n",
    "        'Teacher Forcing Rate': teacher_forcing_rate,\n",
    "        'Attention Matrix': wandb.Image(attention_fig)\n",
    "    })\n",
    "\n",
    "    # Optional: Scheduler Step / Teacher Force Schedule Step\n",
    "    scheduler.step(val_ld)\n",
    "    teacher_forcing_rate = step_teacher_forcing_rate(epoch, teacher_forcing_rate)\n",
    "\n",
    "    save_model(model, optimizer, scheduler, ['val_ld', val_ld], epoch, epoch_model_path)\n",
    "    wandb.save(epoch_model_path)\n",
    "    print(\"Saved epoch model\")\n",
    "\n",
    "    if val_ld <= min_val_ld:\n",
    "        min_val_ld = val_ld\n",
    "        save_model(model, optimizer, scheduler, ['val_ld', val_ld], epoch, best_model_path)\n",
    "        wandb.save(best_model_path)\n",
    "        print(\"Saved best model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-02T19:15:20.841205158Z",
     "start_time": "2023-05-02T19:15:20.837893465Z"
    },
    "id": "hgFYFaBGeBqM"
   },
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloader):\n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    for i, (x, lx) in enumerate(tqdm(dataloader, desc=\"Running Test\")):\n",
    "        x, lx = x.to(DEVICE), lx\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            raw_predictions, _ = model(x, lx, y=None)\n",
    "\n",
    "        # Greedy Decoding\n",
    "        # Get the most likely character from each distribution in the batch\n",
    "        greedy_predictions = torch.argmax(raw_predictions, dim=2)\n",
    "\n",
    "        for prediction_i in greedy_predictions:\n",
    "            char = indices_to_chars(prediction_i, VOCAB)\n",
    "            chars = ''.join(char)\n",
    "            predictions.append(chars)\n",
    "\n",
    "        del x, lx, raw_predictions, greedy_predictions\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.13 / client 1.5.8)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 289k/289k [00:00<00:00, 555kB/s]\n",
      "Successfully submitted to Attention-Based Speech Recognition (Slack)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "predictions = test(model, test_loader)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['label'] = predictions\n",
    "df['index'] = df.index\n",
    "df.to_csv('submission.csv', index=False)\n",
    "\n",
    "!kaggle competitions submit -c attention-based-speech-recognition-slack -f submission.csv -m \"I made it!\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
